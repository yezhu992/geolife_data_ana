{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>数据提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_lable_dir=[]\n",
    "# 指定数据目录路径\n",
    "data_dir = r\"**your_dir**\\Geolife Trajectories 1.3\\Data\"\n",
    "# 遍历所有子文件夹\n",
    "count = 0\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    \n",
    "    # 检查是否是文件夹且名称是三位数字\n",
    "    if os.path.isdir(folder_path) and folder.isdigit() and len(folder) == 3:\n",
    "        label_path = os.path.join(folder_path, \"labels.txt\")\n",
    "        \n",
    "        # 检查labels.txt是否存在\n",
    "        if os.path.exists(label_path):\n",
    "            count += 1\n",
    "            print(f\"文件夹 {folder} 包含 labels.txt\")\n",
    "            data_lable_dir.append(folder_path)\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"文件夹 {folder} 不包含 labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义合并函数\n",
    "def merge_trips(group):\n",
    "    if len(group) > 1:\n",
    "        return pd.DataFrame({\n",
    "            # 'Date': [group['Date'].iloc[0]],\n",
    "            'Start Time': [group['Start Time'].min()],\n",
    "            'End Time': [group['End Time'].max()],\n",
    "            'Transportation Mode': ['-'.join(group['Transportation Mode'])],\n",
    "            'time_diff': [group['time_diff'].iloc[-1]]\n",
    "        })\n",
    "    return group\n",
    "\n",
    "# 获取所有用户目录\n",
    "user_dirs = data_lable_dir\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for user_dir in tqdm(user_dirs,total=len(user_dirs)):\n",
    "    all_results = []\n",
    "    # 处理轨迹数据\n",
    "    traj_dir = os.path.join(user_dir, 'trajectory')\n",
    "    plt_files = [f for f in os.listdir(traj_dir) if f.endswith('.plt')]\n",
    "    user_id = user_dir.split('\\\\')[-1]\n",
    "    # 按顺序处理每个轨迹文件\n",
    "    for plt_file in plt_files:\n",
    "        traj = pd.read_csv(os.path.join(traj_dir, plt_file),\n",
    "                          skiprows=6,\n",
    "                          header=None,\n",
    "                          names=['latitude', 'longitude', 'zero', 'altitude', 'date_days', 'date', 'time'])\n",
    "        \n",
    "        # 处理datetime和分段\n",
    "        traj['datetime'] = traj['date'].astype(str) + ' ' + traj['time'].astype(str)\n",
    "        traj['datetime'] = pd.to_datetime(traj['datetime'])\n",
    "        traj['time_diff'] = traj['datetime'].diff().dt.total_seconds()\n",
    "        traj['segment'] = (traj['time_diff'] >= 60).cumsum()\n",
    "        traj['user'] = user_id  # 使用目录名作为用户ID\n",
    "        \n",
    "        # 分组处理\n",
    "        result = traj.groupby(['user', 'segment'], group_keys=False).apply(\n",
    "            lambda x: {\n",
    "                'user': f\"{user_id}_{x.name[1]}\",\n",
    "                'start_time': x['datetime'].min(),\n",
    "                'end_time': x['datetime'].max(),\n",
    "                'traj': [[lon, lat, str(time)] for lon, lat, time in zip(x['longitude'], x['latitude'], x['datetime'])]\n",
    "            }, include_groups=False\n",
    "        ).reset_index(drop=True)\n",
    "        all_results.extend(result.tolist())\n",
    "\n",
    "    # 将所有结果合并到一个DataFrame\n",
    "    result_df = pd.DataFrame(all_results)\n",
    "    result_df.to_csv(os.path.join(user_dir,'Trajectory/session_traj.csv'), index=False)\n",
    "    # 处理标签数据\n",
    "    label_file = os.path.join(user_dir, 'labels.txt')\n",
    "    label_data = pd.read_csv(label_file, sep='\\t')\n",
    "    \n",
    "    # 合并日期和时间列\n",
    "\n",
    "    # 这个是处理微软单独发布的transportation_mode版本的（日期与时间是分开的）\n",
    "    # label_data['Start Time'] = label_data['Date']+' '+label_data['Start Time']\n",
    "    # label_data['End Time'] = label_data['Date']+' '+label_data['End Time']\n",
    "\n",
    "    label_data['Start Time'] = pd.to_datetime(label_data['Start Time'], format='mixed', dayfirst=False)\n",
    "    label_data['End Time'] = pd.to_datetime(label_data['End Time'], format='mixed', dayfirst=False)\n",
    "    \n",
    "    \"\"\"这是对于有连环出行链需求的研究进行的标签合并的工作：（譬如一个出行里是连续的出行，但是标签是分开的）\n",
    "        例如：一个人的出行链是 步行-骑行-驾车 标签需要合并（步行-骑行-驾车）\n",
    "    \"\"\"\n",
    "    ################################################################################################\n",
    "    ################################################################################################\n",
    "    # # 计算时间差\n",
    "    # label_data['time_diff'] = label_data['Start Time'].shift(-1) - label_data['End Time']\n",
    "    \n",
    "    # # 标记需要合并的行\n",
    "    # label_data['to_merge'] = ((label_data['time_diff'].shift(1) > pd.Timedelta(-1)) & \n",
    "    #                         (label_data['time_diff'].shift(1) < pd.Timedelta('60s'))) | \\\n",
    "    #                         ((label_data['time_diff'] > pd.Timedelta(-1)) & \n",
    "    #                         (label_data['time_diff'] < pd.Timedelta('60s')))\n",
    "    \n",
    "    # # 创建分组\n",
    "    # label_data['group'] = (~label_data['to_merge'] | \n",
    "    #                     ((label_data['to_merge'].shift(1)==False) & \n",
    "    #                     (label_data['to_merge']))).cumsum()\n",
    "    \n",
    "    # # 应用合并\n",
    "    # merged_labels = label_data.groupby('group', group_keys=False).apply(merge_trips).reset_index(drop=True)\n",
    "    # if 'to_merge' in merged_labels.columns:\n",
    "    #     merged_labels = merged_labels.drop(columns=['to_merge', 'group'])\n",
    "    ################################################################################################\n",
    "    ################################################################################################\n",
    "    merged_labels = label_data\n",
    "\n",
    "    result_df['mode'] = None\n",
    "    # 对每个轨迹段进行模糊匹配\n",
    "    for idx, row in result_df.iterrows():\n",
    "        # 时间范围扩大5分钟\n",
    "        start_lower = row['start_time'] - pd.Timedelta(minutes=5)\n",
    "        start_upper = row['start_time'] + pd.Timedelta(minutes=5)\n",
    "        end_lower = row['end_time'] - pd.Timedelta(minutes=5)\n",
    "        end_upper = row['end_time'] + pd.Timedelta(minutes=5)\n",
    "        \n",
    "        # 查找匹配的label\n",
    "        matched = merged_labels[\n",
    "            (\n",
    "                ((merged_labels['Start Time'] >= start_lower) & \n",
    "            (merged_labels['Start Time'] <= start_upper)) &\n",
    "            ((merged_labels['End Time'] >= end_lower) & \n",
    "            (merged_labels['End Time'] <= end_upper))\n",
    "            )|\n",
    "            (\n",
    "                (merged_labels['Start Time']<=row['start_time'])&\n",
    "                (merged_labels['End Time']>=row['end_time'])\n",
    "            )\n",
    "        ]\n",
    "        # 如果是连续的出行链\n",
    "    ################################################################################################\n",
    "        \"\"\"\n",
    "            matched = merged_labels[\n",
    "                ((merged_labels['Start Time'] >= start_lower) & \n",
    "            (merged_labels['Start Time'] <= start_upper)) &\n",
    "            ((merged_labels['End Time'] >= end_lower) & \n",
    "            (merged_labels['End Time'] <= end_upper))\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # 匹配轨迹长度约为2k条，注意如果是要筛选出行链的话，前面对每一个session_traj的时间阈值要放大一点\n",
    "        # 2k的记录是阈值2分钟的结果，可以尝试5分钟\n",
    "    ################################################################################################\n",
    "        # 如果找到匹配项，取第一个匹配的mode\n",
    "        if not matched.empty:\n",
    "            result_df.at[idx, 'mode'] = matched.iloc[0]['Transportation Mode']\n",
    "    corrected_match_length = len(result_df[result_df['mode'].notna()])\n",
    "    data_length = len(result_df)\n",
    "    print(f'{user_id}成功匹配率: {corrected_match_length / data_length * 100:.2f}%')\n",
    "\n",
    "    out_df = result_df[result_df['mode'].notna()].copy()\n",
    "    \n",
    "    out_df.to_csv(os.path.join(user_dir, 'match_mode.csv'), index=False)\n",
    "    print(f'{user_id}匹配保存成功')\n",
    "    import ast\n",
    "def parse_traj(traj_str):\n",
    "    return ast.literal_eval(traj_str)\n",
    "user_match_modes = []\n",
    "for user_dir in user_dirs:\n",
    "    user_match_modes.append(os.path.join(user_dir, 'match_mode.csv'))\n",
    "    \n",
    "combine_match_modes = pd.concat([\n",
    "    pd.read_csv(\n",
    "        f, \n",
    "        converters={'traj': parse_traj}  # 对traj列应用转换函数\n",
    "    ) for f in user_match_modes\n",
    "])\n",
    "traj_match_length = len(combine_match_modes)\n",
    "print(f'成功匹配方式轨迹数据量：{traj_match_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_match_modes['start_point'] = combine_match_modes['traj'].apply(lambda x: x[0][:2])\n",
    "combine_match_modes.reset_index(drop=True, inplace=True)\n",
    "combine_match_modes.to_csv('match_mode_combine.csv', index=False)\n",
    "# 筛选北京轨迹\n",
    "inbeijing = combine_match_modes['start_point'].apply(lambda x: (117.5>=x[0]>=115.5)&(41.6>=x[1]>=39.4))\n",
    "combine_match_modes_bj = combine_match_modes[inbeijing]\n",
    "combine_match_modes_bj.reset_index(drop=True, inplace=True)\n",
    "combine_match_modes_bj.to_csv('match_mode_combine_bj.csv', index=False)\n",
    "print('北京轨迹筛选完成')\n",
    "print(f'北京轨迹数量：{len(combine_match_modes_bj)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
